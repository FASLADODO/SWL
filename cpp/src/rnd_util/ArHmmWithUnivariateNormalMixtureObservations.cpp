#include "swl/Config.h"
#include "swl/rnd_util/ArHmmWithUnivariateNormalMixtureObservations.h"
#include "RndUtilLocalApi.h"
#include <boost/numeric/ublas/matrix_proxy.hpp>
#include <boost/math/distributions/normal.hpp>  // for normal distribution.
#include <boost/random/normal_distribution.hpp>
#include <boost/random/variate_generator.hpp>
#include <numeric>
#include <iostream>
#include <stdexcept>
#include <cassert>


#if defined(_DEBUG) && defined(__SWL_CONFIG__USE_DEBUG_NEW)
#include "swl/ResourceLeakageCheck.h"
#define new DEBUG_NEW
#endif


namespace swl {

// [ref] swl/src/rnd_util/RndUtilLocalApi.cpp.
bool solve_linear_equations_by_lu(const boost::numeric::ublas::matrix<double> &m, boost::numeric::ublas::vector<double> &x);

ArHmmWithUnivariateNormalMixtureObservations::ArHmmWithUnivariateNormalMixtureObservations(const size_t K, const size_t C, const size_t P)
: base_type(K, 1, C), P_(P), coeffs_(boost::extents[K][C]), sigmas_(boost::extents[K][C]),  // 0-based index.
  coeffs_conj_(),
  baseGenerator_()
{
	assert(P_ > 0);

	for (size_t k = 0; k < K; ++k)
		for (size_t c = 0; c < C; ++c)
		{
			coeffs_[k][c] = dvector_type(P, 0.0);
			sigmas_[k][c] = 0.0;
		}
}

ArHmmWithUnivariateNormalMixtureObservations::ArHmmWithUnivariateNormalMixtureObservations(const size_t K, const size_t C, const size_t P, const dvector_type &pi, const dmatrix_type &A, const dmatrix_type &alphas, const boost::multi_array<dvector_type, 2> &coeffs, const boost::multi_array<double, 2> &sigmas)
: base_type(K, 1, C, pi, A, alphas), P_(P), coeffs_(coeffs), sigmas_(sigmas),
  coeffs_conj_(),
  baseGenerator_()
{
	assert(P_ > 0);
}

ArHmmWithUnivariateNormalMixtureObservations::ArHmmWithUnivariateNormalMixtureObservations(const size_t K, const size_t C, const size_t P, const dvector_type *pi_conj, const dmatrix_type *A_conj, const dmatrix_type *alphas_conj, const boost::multi_array<dvector_type, 2> *coeffs_conj)
: base_type(K, 1, C, pi_conj, A_conj, alphas_conj), P_(P), coeffs_(boost::extents[K][C]), sigmas_(boost::extents[K][C]),
  coeffs_conj_(coeffs_conj),
  baseGenerator_()
{
	// FIXME [modify] >>
	throw std::runtime_error("Not yet implemented");

	assert(P_ > 0);

	for (size_t k = 0; k < K; ++k)
		for (size_t c = 0; c < C; ++c)
		{
			coeffs_[k][c] = dvector_type(P, 0.0);
			sigmas_[k][c] = 0.0;
		}
}

ArHmmWithUnivariateNormalMixtureObservations::~ArHmmWithUnivariateNormalMixtureObservations()
{
}

void ArHmmWithUnivariateNormalMixtureObservations::doEstimateObservationDensityParametersByML(const size_t N, const unsigned int state, const dmatrix_type &observations, const dmatrix_type &gamma, const double denominatorA)
{
	assert(N > P_);

	const double eps = 1e-50;
	size_t c, n;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	dmatrix_type zeta(N, C_, 0.0);
	{
		double denominator;
		double val;
		for (n = 0; n < N; ++n)
		{
			//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);

			denominator = 0.0;
			for (c = 0; c < C_; ++c)
			{
				//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
				val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observations));

				zeta(n, c) = val;
				denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
			}

			if (denominator < eps)
			{
				// FIXME [check] >>
				//	because responsibilities, gamma(y_nc) means membership, the values may become zero if the corresponding mixture model doesn't generate a sample.
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.0;
			}
			else
			{
#if 0
				val = 0.999 * gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.001 + val * zeta(n, c);
#else
				val = gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) *= val;
#endif
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	const double sumGamma = denominatorA + gamma(N-1, state);
	assert(std::fabs(sumGamma) >= eps);
	const double factorAlpha = 0.999 / sumGamma;

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	// calculate autocovariance functions.
	dmatrix_type autocovariance(N, P_ + 1, 0.0);  // autocovariance function.
	double meanWinObs;
	for (n = 0; n < N; ++n)
	{
		for (w = -W; w <= W; ++w)
		{
			// TODO [check] >> which one is better?
			//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(N)) ? 0.0 : observations(n + w, 0);
			winObs(w + W) = int(n) + w < 0 ? observations(0, 0) : (int(n) + w >= int(N) ? observations(N - 1, 0) : observations(n + w, 0));
		}

		meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
		// zero mean observations.
		for (j = 0; j < L; ++j)
			winObs(j) -= meanWinObs;

		for (p = 0; p <= P_; ++p)
			for (j = 0; j < L - p; ++j)
				autocovariance(n, p) += winObs(j) * winObs(j + p);
	}

	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		sumZeta = 0.0;
		for (n = 0; n < N; ++n)
			sumZeta += zeta(n, c);
		assert(std::fabs(sumZeta) >= eps);

		// reestimate mixture coefficients(weights).
		alphas_(state, c) = 0.001 + factorAlpha * sumZeta;

		autocovariance_bar.clear();
		for (p = 0; p <= P_; ++p)
		{
			for (n = 0; n < N; ++n)
				autocovariance_bar(p) += zeta(n, c) * autocovariance(n, p);
			autocovariance_bar(p) /= sumZeta;
		}

		// reestimate the autoregression coefficients.
		for (p = 0; p < P_; ++p)
		{
			x(p) = autocovariance_bar(p + 1);
			for (j = 0; j < p; ++j)
				A(p, j) = autocovariance_bar(p - j);
			for (j = p; j < P_; ++j)
				A(p, j) = autocovariance_bar(j - p);
		}

		if (solve_linear_equations_by_lu(A, x))
		{
			dvector_type &coeff = coeffs_[state][c];
			coeff = x;

			// reestimate the variances of the input noise process.
			double &sigma2 = sigmas_[state][c];
			sigma2 = autocovariance_bar(0);
			for (p = 0; p < P_; ++p)
				sigma2 -= coeff(p) * autocovariance_bar(p + 1);
			assert(sigma2 > 0.0);
		}
		else
		{
			assert(false);
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be positive.
}

void ArHmmWithUnivariateNormalMixtureObservations::doEstimateObservationDensityParametersByML(const std::vector<size_t> &Ns, const unsigned int state, const std::vector<dmatrix_type> &observationSequences, const std::vector<dmatrix_type> &gammas, const size_t R, const double denominatorA)
{
	const double eps = 1e-50;
	size_t c, n, r;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	std::vector<dmatrix_type> zetas;
	zetas.reserve(R);
	for (r = 0; r < R; ++r)
		zetas.push_back(dmatrix_type(Ns[r], C_, 0.0));

	{
		double denominator;
		double val;
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &gammar = gammas[r];
			dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);

				denominator = 0.0;
				for (c = 0; c < C_; ++c)
				{
					//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
					val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr));

					zetar(n, c) = val;
					denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
				}

				if (denominator < eps)
				{
					// FIXME [check] >>
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.0;
				}
				else
				{
#if 0
					val = 0.999 * gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.001 + val * zetar(n, c);
#else
					val = gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) *= val;
#endif
				}
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	double sumGamma = denominatorA;
	for (r = 0; r < R; ++r)
		sumGamma += gammas[r](Ns[r]-1, state);
	assert(std::fabs(sumGamma) >= eps);
	const double factorAlpha = 0.999 / sumGamma;

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	std::vector<dmatrix_type> autocovariances(R);  // autocovariance functions.
	double meanWinObs;
	for (r = 0; r < R; ++r)
	{
		assert(Ns[r] > P_);

		const dmatrix_type &observationr = observationSequences[r];
		//const dmatrix_type &zetar = zetas[r];
		dmatrix_type &autocovariancer = autocovariances[r];

		// calculate autocovariance functions.
		autocovariancer.resize(Ns[r], P_ + 1, false);
		autocovariancer.clear();
		for (n = 0; n < Ns[r]; ++n)
		{
			for (w = -W; w <= W; ++w)
			{
				// TODO [check] >> which one is better?
				//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(Ns[r])) ? 0.0 : observationr(n + w, 0);
				winObs(w + W) = int(n) + w < 0 ? observationr(0, 0) : (int(n) + w >= int(Ns[r]) ? observationr(Ns[r] - 1, 0) : observationr(n + w, 0));
			}

			meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
			// zero mean observations.
			for (j = 0; j < L; ++j)
				winObs(j) -= meanWinObs;

			for (p = 0; p <= P_; ++p)
				for (j = 0; j < L - p; ++j)
					autocovariancer(n, p) += winObs(j) * winObs(j + p);
		}
	}

	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		sumZeta = 0.0;
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
				sumZeta += zetar(n, c);
		}
		assert(std::fabs(sumZeta) >= eps);

		// reestimate mixture coefficients(weights).
		alphas_(state, c) = 0.001 + factorAlpha * sumZeta;

		autocovariance_bar.clear();
		for (p = 0; p <= P_; ++p)
		{
			for (r = 0; r < R; ++r)
			{
				const dmatrix_type &zetar = zetas[r];
				const dmatrix_type &autocovariancer = autocovariances[r];

				for (n = 0; n < Ns[r]; ++n)
					autocovariance_bar(p) += zetar(n, c) * autocovariancer(n, p);
			}

			autocovariance_bar(p) /= sumZeta;
		}

		// reestimate the autoregression coefficients.
		for (p = 0; p < P_; ++p)
		{
			x(p) = autocovariance_bar(p + 1);
			for (j = 0; j < p; ++j)
				A(p, j) = autocovariance_bar(p - j);
			for (j = p; j < P_; ++j)
				A(p, j) = autocovariance_bar(j - p);
		}

		if (solve_linear_equations_by_lu(A, x))
		{
			dvector_type &coeff = coeffs_[state][c];
			coeff = x;

			// reestimate the variances of the input noise process.
			double &sigma2 = sigmas_[state][c];
			sigma2 = autocovariance_bar(0);
			for (p = 0; p < P_; ++p)
				sigma2 -= coeff(p) * autocovariance_bar(p + 1);
			assert(sigma2 > 0.0);
		}
		else
		{
			assert(false);
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be positive.
}

void ArHmmWithUnivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingConjugatePrior(const size_t N, const unsigned int state, const dmatrix_type &observations, const dmatrix_type &gamma, const double denominatorA)
{
	// FIXME [modify] >>
	throw std::runtime_error("Not yet implemented");

#if 0
	const double eps = 1e-50;
	size_t c, n;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	dmatrix_type zeta(N, C_, 0.0);
	{
		double denominator;
		double val;
		for (n = 0; n < N; ++n)
		{
			//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);

			denominator = 0.0;
			for (c = 0; c < C_; ++c)
			{
				//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
				val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observations));

				zeta(n, c) = val;
				denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
			}

			if (denominator < eps)
			{
				// FIXME [check] >>
				//	because responsibilities, gamma(y_nc) means membership, the values may become zero if the corresponding mixture model doesn't generate a sample.
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.0;
			}
			else
			{
#if 0
				val = 0.999 * gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.001 + val * zeta(n, c);
#else
				val = gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) *= val;
#endif
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	const double sumGamma = denominatorA + gamma(N-1, state);
	//assert(std::fabs(sumGamma) >= eps);
	double denominatorAlpha0 = -double(C_);
	for (c = 0; c < C_; ++c)
		denominatorAlpha0 += (*alphas_conj_)(state, c);
	const double factorAlpha = 0.999 / (sumGamma + denominatorAlpha0);

	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		sumZeta = 0.0;
		for (n = 0; n < N; ++n)
			sumZeta += zeta(n, c);
		//assert(std::fabs(sumZeta) >= eps);

		// reestimate mixture coefficients(weights).
		alphas_(state, c) = 0.001 + factorAlpha * (sumZeta + (*alphas_conj_)(state, c) - 1.0);

		// reestimate observation(emission) distribution in each state.
		double &mu = mus_(state, c);
		mu = (*betas_conj_)(state, c) * (*mus_conj_)(state, c);
		for (n = 0; n < N; ++n)
			mu += zeta(n, c) * observations(n, 0);
		mu = 0.001 + 0.999 * mu / (sumZeta + (*betas_conj_)(state, c));

		//
		double &sigma = sigmas_(state, c);
		sigma = (*sigmas_conj_)(state, c) + (*betas_conj_)(state, c) * (mu - (*mus_conj_)(state, c)) * (mu - (*mus_conj_)(state, c));
		for (n = 0; n < N; ++n)
			sigma += zeta(n, c) * (observations(n, 0) - mu) * (observations(n, 0) - mu);
		sigma = 0.001 + 0.999 * std::sqrt(sigma / (sumZeta + (*nus_conj_)(state, c) - D_));
		assert(sigma > 0.0);
	}

	// POSTCONDITIONS [] >>
	//	-. all standard deviations have to be positive.
#endif
}

void ArHmmWithUnivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingConjugatePrior(const std::vector<size_t> &Ns, const unsigned int state, const std::vector<dmatrix_type> &observationSequences, const std::vector<dmatrix_type> &gammas, const size_t R, const double denominatorA)
{
	// FIXME [modify] >>
	throw std::runtime_error("Not yet implemented");

#if 0
	const double eps = 1e-50;
	size_t c, n, r;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	std::vector<dmatrix_type> zetas;
	zetas.reserve(R);
	for (r = 0; r < R; ++r)
		zetas.push_back(dmatrix_type(Ns[r], C_, 0.0));

	{
		double denominator;
		double val;
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &gammar = gammas[r];
			dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);

				denominator = 0.0;
				for (c = 0; c < C_; ++c)
				{
					//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
					val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr));

					zetar(n, c) = val;
					denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
				}

				if (denominator < eps)
				{
					// FIXME [check] >>
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.0;
				}
				else
				{
#if 0
					val = 0.999 * gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.001 + val * zetar(n, c);
#else
					val = gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) *= val;
#endif
				}
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	double sumGamma = denominatorA;
	for (r = 0; r < R; ++r)
		sumGamma += gammas[r](Ns[r]-1, state);
	//assert(std::fabs(sumGamma) >= eps);
	double denominatorAlpha0 = -double(C_);
	for (c = 0; c < C_; ++c)
		denominatorAlpha0 += (*alphas_conj_)(state, c);
	const double factorAlpha = 0.999 / (sumGamma + denominatorAlpha0);

	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		sumZeta = 0.0;
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
				sumZeta += zetar(n, c);
		}
		//assert(std::fabs(sumZeta) >= eps);

		// reestimate mixture coefficients(weights).
		alphas_(state, c) = 0.001 + factorAlpha * (sumZeta + (*alphas_conj_)(state, c) - 1.0);

		// reestimate observation(emission) distribution in each state.
		double &mu = mus_(state, c);
		mu = (*betas_conj_)(state, c) * (*mus_conj_)(state, c);
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
				mu += zetar(n, c) * observationr(n, 0);
		}
		mu = 0.001 + 0.999 * mu / (sumZeta + (*betas_conj_)(state, c));

		//
		double &sigma = sigmas_(state, c);
		sigma = (*sigmas_conj_)(state, c) + (*betas_conj_)(state, c) * (mu - (*mus_conj_)(state, c)) * (mu - (*mus_conj_)(state, c));
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
				sigma += zetar(n, c) * (observationr(n, 0) - mu) * (observationr(n, 0) - mu);
		}
		sigma = 0.001 + 0.999 * std::sqrt(sigma / (sumZeta + (*nus_conj_)(state, c) - D_));
		assert(sigma > 0.0);
	}

	// POSTCONDITIONS [] >>
	//	-. all standard deviations have to be positive.
#endif
}

void ArHmmWithUnivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingEntropicPrior(const size_t N, const unsigned int state, const dmatrix_type &observations, const dmatrix_type &gamma, const double z, const bool doesTrimParameter, const double terminationTolerance, const size_t maxIteration, const double /*denominatorA*/)
{
	const double eps = 1e-40;
	size_t c, n;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	dmatrix_type zeta(N, C_, 0.0);
	{
		double denominator;
		double val;
		for (n = 0; n < N; ++n)
		{
			//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);

			denominator = 0.0;
			for (c = 0; c < C_; ++c)
			{
				// TODO [check] >> we need to check if a component is trimmed or not.
				//	Here, we use the value of alpha in order to check if a component is trimmed or not.
				//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
				val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observations));

				zeta(n, c) = val;
				denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
			}

			if (denominator < eps)
			{
				// FIXME [check] >>
				//	because responsibilities, gamma(y_nc) means membership, the values may become zero if the corresponding mixture model doesn't generate a sample.
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.0;
			}
			else
			{
#if 0
				val = 0.999 * gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.001 + val * zeta(n, c);
#else
				val = gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) *= val;
#endif
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	{
		// compute expected sufficient statistics (ESS).
		std::vector<double> omega(C_, 0.0), theta(C_, 0.0);
		for (c = 0; c < C_; ++c)
		{
			omega[c] = 0.0;
			for (n = 0; n < N; ++n)
				omega[c] += zeta(n, c);
		}

		// reestimate mixture coefficients(weights).
		double entropicMAPLogLikelihood = 0.0;
		const bool retval = computeMAPEstimateOfMultinomialUsingEntropicPrior(omega, z, theta, entropicMAPLogLikelihood, terminationTolerance, maxIteration, false);
		assert(retval);

		// trim mixture coefficients(weights).
		if (doesTrimParameter && std::fabs(z - 1.0) <= eps)
		{
			dmatrix_type prob(N, C_, 0.0);
			for (n = 0; n < N; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);
				for (c = 0; c < C_; ++c)
				{
					// TODO [check] >> we need to check if a component is trimmed or not.
					//	Here, we use the value of alpha in order to check if a component is trimmed or not.
					//prob(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, obs);
					prob(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, n, observations);
				}
			}

			size_t i;
			double grad;
			bool isNormalized = false;
			double numerator, denominator;
			for (c = 0; c < C_; ++c)
			{
				if (alphas_(state, c) >= eps)  // not yet trimmed.
				{
					grad = 0.0;
					for (n = 0; n < N; ++n)
					{
						numerator = prob(n, c);
						if (std::fabs(numerator) >= eps)
						{
							denominator = 0.0;
							for (i = 0; i < C_; ++i)
								denominator += prob(n, i) * theta[i];

							assert(std::fabs(denominator) >= eps);
							grad += numerator / denominator;
						}
						//else grad += 0.0;
					}

					if (theta[c] <= std::exp(-grad / z))
					{
						theta[c] = 0.0;
						isNormalized = true;
					}
				}
			}

			if (isNormalized)
			{
				double sumTheta = std::accumulate(theta.begin(), theta.end(), 0.0);
				assert(std::fabs(sumTheta) >= eps);
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c] / sumTheta;
			}
			else
			{
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c];
			}
		}
		else
		{
			for (c = 0; c < C_; ++c)
				alphas_(state, c) = theta[c];
		}
	}

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	// calculate autocovariance functions.
	dmatrix_type autocovariance(N, P_ + 1, 0.0);  // autocovariance function.
	double meanWinObs;
	for (n = 0; n < N; ++n)
	{
		for (w = -W; w <= W; ++w)
		{
			// TODO [check] >> which one is better?
			//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(N)) ? 0.0 : observations(n + w, 0);
			winObs(w + W) = int(n) + w < 0 ? observations(0, 0) : (int(n) + w >= int(N) ? observations(N - 1, 0) : observations(n + w, 0));
		}

		meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
		// zero mean observations.
		for (j = 0; j < L; ++j)
			winObs(j) -= meanWinObs;

		for (p = 0; p <= P_; ++p)
			for (j = 0; j < L - p; ++j)
				autocovariance(n, p) += winObs(j) * winObs(j + p);
	}

	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		if (alphas_(state, c) < eps)  // already trimmed.
		{
			coeffs_[state][c].clear();
			sigmas_[state][c] = 0.0;
		}
		else
		{
			sumZeta = 0.0;
			for (n = 0; n < N; ++n)
				sumZeta += zeta(n, c);
			assert(std::fabs(sumZeta) >= eps);

			autocovariance_bar.clear();
			for (p = 0; p <= P_; ++p)
			{
				for (n = 0; n < N; ++n)
					autocovariance_bar(p) += zeta(n, c) * autocovariance(n, p);
				autocovariance_bar(p) /= sumZeta;
			}

			// reestimate the autoregression coefficients.
			for (p = 0; p < P_; ++p)
			{
				x(p) = autocovariance_bar(p + 1);
				for (j = 0; j < p; ++j)
					A(p, j) = autocovariance_bar(p - j);
				for (j = p; j < P_; ++j)
					A(p, j) = autocovariance_bar(j - p);
			}

			if (solve_linear_equations_by_lu(A, x))
			{
				dvector_type &coeff = coeffs_[state][c];
				coeff = x;

				// reestimate the variances of the input noise process.
				double &sigma2 = sigmas_[state][c];
				sigma2 = autocovariance_bar(0);
				for (p = 0; p < P_; ++p)
					sigma2 -= coeff(p) * autocovariance_bar(p + 1);
				assert(sigma2 > 0.0);
			}
			else
			{
				assert(false);
			}
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be positive.
}

void ArHmmWithUnivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingEntropicPrior(const std::vector<size_t> &Ns, const unsigned int state, const std::vector<dmatrix_type> &observationSequences, const std::vector<dmatrix_type> &gammas, const double z, const bool doesTrimParameter, const double terminationTolerance, const size_t maxIteration, const size_t R, const double /*denominatorA*/)
{
	const double eps = 1e-50;
	size_t c, n, r;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	std::vector<dmatrix_type> zetas;
	zetas.reserve(R);
	for (r = 0; r < R; ++r)
		zetas.push_back(dmatrix_type(Ns[r], C_, 0.0));

	{
		double denominator;
		double val;
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &gammar = gammas[r];
			dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);

				denominator = 0.0;
				for (c = 0; c < C_; ++c)
				{
					// TODO [check] >> we need to check if a component is trimmed or not.
					//	Here, we use the value of alpha in order to check if a component is trimmed or not.
					//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
					val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr));

					zetar(n, c) = val;
					denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
				}

				if (denominator < eps)
				{
					// FIXME [check] >>
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.0;
				}
				else
				{
#if 0
					val = 0.999 * gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.001 + val * zetar(n, c);
#else
					val = gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) *= val;
#endif
				}
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	{
		// compute expected sufficient statistics (ESS).
		std::vector<double> omega(C_, 0.0), theta(C_, 0.0);
		for (c = 0; c < C_; ++c)
		{
			omega[c] = 0.0;
			for (r = 0; r < R; ++r)
			{
				const dmatrix_type &zetar = zetas[r];
				for (n = 0; n < Ns[r]; ++n)
					omega[c] += zetar(n, c);
			}
		}

		// reestimate mixture coefficients(weights).
		double entropicMAPLogLikelihood = 0.0;
		const bool retval = computeMAPEstimateOfMultinomialUsingEntropicPrior(omega, z, theta, entropicMAPLogLikelihood, terminationTolerance, maxIteration, false);
		assert(retval);

		// trim mixture coefficients(weights).
		if (doesTrimParameter && std::fabs(z - 1.0) <= eps)
		{
			std::vector<dmatrix_type> probs(R);
			for (r = 0; r < R; ++r)
			{
				const size_t &Nr = Ns[r];
				const dmatrix_type &observationr = observationSequences[r];

				dmatrix_type &probr = probs[r];
				probr.resize(Nr, C_);
				for (n = 0; n < Nr; ++n)
				{
					//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);
					for (c = 0; c < C_; ++c)
					{
						// TODO [check] >> we need to check if a component is trimmed or not.
						//	Here, we use the value of alpha in order to check if a component is trimmed or not.
						//probr(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, obs);
						probr(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr);
					}
				}
			}

			size_t i;
			double grad;
			bool isNormalized = false;
			double numerator, denominator;
			for (c = 0; c < C_; ++c)
			{
				if (alphas_(state, c) >= eps)  // not yet trimmed.
				{
					grad = 0.0;
					for (r = 0; r < R; ++r)
					{
						const size_t &Nr = Ns[r];
						const dmatrix_type &probr = probs[r];
						for (n = 0; n < Nr; ++n)
						{
							numerator = probr(n, c);
							if (std::fabs(numerator) >= eps)
							{
								denominator = 0.0;
								for (i = 0; i < C_; ++i)
									denominator += probr(n, i) * theta[i];

								assert(std::fabs(denominator) >= eps);
								grad += numerator / denominator;
							}
							//else grad += 0.0;
						}
					}

					if (theta[c] <= std::exp(-grad / z))
					{
						theta[c] = 0.0;
						isNormalized = true;
					}
				}
			}

			if (isNormalized)
			{
				double sumTheta = std::accumulate(theta.begin(), theta.end(), 0.0);
				assert(std::fabs(sumTheta) >= eps);
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c] / sumTheta;
			}
			else
			{
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c];
			}
		}
		else
		{
			for (c = 0; c < C_; ++c)
				alphas_(state, c) = theta[c];
		}
	}

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	std::vector<dmatrix_type> autocovariances(R);  // autocovariance functions.
	double meanWinObs;
	for (r = 0; r < R; ++r)
	{
		assert(Ns[r] > P_);

		const dmatrix_type &observationr = observationSequences[r];
		//const dmatrix_type &zetar = zetas[r];
		dmatrix_type &autocovariancer = autocovariances[r];

		// calculate autocovariance functions.
		autocovariancer.resize(Ns[r], P_ + 1, false);
		autocovariancer.clear();
		for (n = 0; n < Ns[r]; ++n)
		{
			for (w = -W; w <= W; ++w)
			{
				// TODO [check] >> which one is better?
				//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(Ns[r])) ? 0.0 : observationr(n + w, 0);
				winObs(w + W) = int(n) + w < 0 ? observationr(0, 0) : (int(n) + w >= int(Ns[r]) ? observationr(Ns[r] - 1, 0) : observationr(n + w, 0));
			}

			meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
			// zero mean observations.
			for (j = 0; j < L; ++j)
				winObs(j) -= meanWinObs;

			for (p = 0; p <= P_; ++p)
				for (j = 0; j < L - p; ++j)
					autocovariancer(n, p) += winObs(j) * winObs(j + p);
		}
	}

	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		if (alphas_(state, c) < eps)  // already trimmed.
		{
			coeffs_[state][c].clear();
			sigmas_[state][c] = 0.0;
		}
		else
		{
			sumZeta = 0.0;
			for (r = 0; r < R; ++r)
			{
				const dmatrix_type &zetar = zetas[r];

				for (n = 0; n < Ns[r]; ++n)
					sumZeta += zetar(n, c);
			}
			assert(std::fabs(sumZeta) > eps);

			//
			autocovariance_bar.clear();
			for (p = 0; p <= P_; ++p)
			{
				for (r = 0; r < R; ++r)
				{
					const dmatrix_type &zetar = zetas[r];
					const dmatrix_type &autocovariancer = autocovariances[r];

					for (n = 0; n < Ns[r]; ++n)
						autocovariance_bar(p) += zetar(n, c) * autocovariancer(n, p);
				}

				autocovariance_bar(p) /= sumZeta;
			}

			// reestimate the autoregression coefficients.
			for (p = 0; p < P_; ++p)
			{
				x(p) = autocovariance_bar(p + 1);
				for (j = 0; j < p; ++j)
					A(p, j) = autocovariance_bar(p - j);
				for (j = p; j < P_; ++j)
					A(p, j) = autocovariance_bar(j - p);
			}

			if (solve_linear_equations_by_lu(A, x))
			{
				dvector_type &coeff = coeffs_[state][c];
				coeff = x;

				// reestimate the variances of the input noise process.
				double &sigma2 = sigmas_[state][c];
				sigma2 = autocovariance_bar(0);
				for (p = 0; p < P_; ++p)
					sigma2 -= coeff(p) * autocovariance_bar(p + 1);
				assert(sigma2 > 0.0);
			}
			else
			{
				assert(false);
			}
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all standard deviations have to be positive.
}

double ArHmmWithUnivariateNormalMixtureObservations::doEvaluateEmissionProbability(const unsigned int state, const dvector_type &observation) const
{
	assert(false);

	return base_type::doEvaluateEmissionProbability(state, observation);
}

double ArHmmWithUnivariateNormalMixtureObservations::doEvaluateEmissionProbability(const unsigned int state, const size_t n, const dmatrix_type &observations) const
{
	return base_type::doEvaluateEmissionProbability(state, n, observations);
}

double ArHmmWithUnivariateNormalMixtureObservations::doEvaluateEmissionMixtureComponentProbability(const unsigned int state, const unsigned int component, const dvector_type &observation) const
{
	assert(false);

	double M = 0.0;

	//const dvector_type &coeff = coeffs_[state][component];
	for (size_t p = 0; p < P_; ++p)
	{
		// FIXME [check] > is it correct?
		// TODO [check] >> which one is better?
		M = 0.0;  //coeff(p) * 0.0;
		//M = coeff(p) * observation(0);
	}

	//boost::math::normal pdf;  // (default mean = zero, and standard deviation = unity).
	boost::math::normal pdf(M, sigmas_[state][component]);

	return boost::math::pdf(pdf, observation(0));
}

double ArHmmWithUnivariateNormalMixtureObservations::doEvaluateEmissionMixtureComponentProbability(const unsigned int state, const unsigned int component, const size_t n, const dmatrix_type &observations) const
{
	double M = 0.0;

	const dvector_type &coeff = coeffs_[state][component];
	for (size_t p = 0; p < P_; ++p)
	{
		// TODO [check] >> which one is better?
		//M = coeff(p) * ((int(n) - int(p) - 1 < 0) ? 0.0 : observations(n-p-1, 0));
		M = coeff(p) * ((int(n) - int(p) - 1 < 0) ? observations(0, 0) : observations(n-p-1, 0));
	}

	//boost::math::normal pdf;  // (default mean = zero, and standard deviation = unity).
	boost::math::normal pdf(M, sigmas_[state][component]);

	return boost::math::pdf(pdf, observations(n, 0));
}

void ArHmmWithUnivariateNormalMixtureObservations::doGenerateObservationsSymbol(const unsigned int state, const size_t n, dmatrix_type &observations) const
{
	const double prob = (double)std::rand() / RAND_MAX;

	double accum = 0.0;
	unsigned int component = (unsigned int)C_;
	for (size_t c = 0; c < C_; ++c)
	{
		accum += alphas_(state, c);
		if (prob < accum)
		{
			component = (unsigned int)c;
			break;
		}
	}

	// TODO [check] >>
	if ((unsigned int)C_ == component)
		component = (unsigned int)(C_ - 1);

	//
	double M = 0.0;

	const dvector_type &coeff = coeffs_[state][component];
	for (size_t p = 0; p < P_; ++p)
	{
		// TODO [check] >> which one is better?
		//M = coeff(p) * ((int(n) - int(p) - 1 < 0) ? 0.0 : observations(n-p-1, 0));
		M = coeff(p) * ((int(n) - int(p) - 1 < 0) ? observations(0, 0) : observations(n-p-1, 0));
	}

	//
	typedef boost::normal_distribution<> distribution_type;
	typedef boost::variate_generator<base_generator_type &, distribution_type> generator_type;

	// TODO [check] >> assume x_-1 = observations(-1, 0) = 0.
	generator_type normal_gen(baseGenerator_, distribution_type(M, sigmas_[state][component]));
	observations(n, 0) = normal_gen();
}

void ArHmmWithUnivariateNormalMixtureObservations::doInitializeRandomSampleGeneration(const unsigned int seed /*= (unsigned int)-1*/) const
{
	if ((unsigned int)-1 != seed)
	{
		std::srand(seed);
		baseGenerator_.seed(seed);
	}
}

bool ArHmmWithUnivariateNormalMixtureObservations::doReadObservationDensity(std::istream &stream)
{
	if (1 != D_) return false;

	std::string dummy;
	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "ar") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "ar") != 0)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "univariate") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "univariate") != 0)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "normal") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "normal") != 0)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "mixture:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "mixture:") != 0)
#endif
		return false;

	// TODO [check] >>
	size_t C;
	stream >> dummy >> C;  // the number of mixture components.
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "C=") != 0 || C_ != C)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "C=") != 0 || C_ != C)
#endif
		return false;

	// TODO [check] >>
	size_t P;
	stream >> dummy >> P;  // the order of autoregressive model.
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "P=") != 0 || P_ != P)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "P=") != 0 || P_ != P)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "alpha:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "alpha:") != 0)
#endif
		return false;

	size_t k, c, p;

	// K x C.
	for (k = 0; k < K_; ++k)
		for (c = 0; c < C_; ++c)
			stream >> alphas_(k, c);

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "coeff:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "coeff:") != 0)
#endif
		return false;

	// K x C x P.
	for (k = 0; k < K_; ++k)
		for (c = 0; c < C_; ++c)
			for (p = 0; p < P_; ++p)
				stream >> coeffs_[k][c](p);

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "sigma:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "sigma:") != 0)
#endif
		return false;

	// K x C.
	for (k = 0; k < K_; ++k)
		for (c = 0; c < C_; ++c)
			stream >> sigmas_[k][c];

	return true;
}

bool ArHmmWithUnivariateNormalMixtureObservations::doWriteObservationDensity(std::ostream &stream) const
{
	stream << "ar univariate normal mixture:" << std::endl;

	stream << "C= " << C_ << std::endl;  // the number of mixture components.

	stream << "P= " << P_ << std::endl;  // the order of autoregressive model.

	size_t k, c, p;

	// K x C.
	stream << "alpha:" << std::endl;
	for (k = 0; k < K_; ++k)
	{
		for (c = 0; c < C_; ++c)
			stream << alphas_(k, c) << ' ';
		stream << std::endl;
	}

	// K x C x P.
	stream << "coeff:" << std::endl;
	for (k = 0; k < K_; ++k)
	{
		for (c = 0; c < C_; ++c)
		{
			for (p = 0; p < P_; ++p)
				stream << coeffs_[k][c](p) << ' ';
			stream << "  ";
		}
		stream << std::endl;
	}

	// K x C.
	stream << "sigma:" << std::endl;
	for (k = 0; k < K_; ++k)
	{
		for (c = 0; c < C_; ++c)
			stream << sigmas_[k][c] << ' ';
		stream << std::endl;
	}

	return true;
}

void ArHmmWithUnivariateNormalMixtureObservations::doInitializeObservationDensity(const std::vector<double> &lowerBoundsOfObservationDensity, const std::vector<double> &upperBoundsOfObservationDensity)
{
	// PRECONDITIONS [] >>
	//	-. std::srand() has to be called before this function is called.

	// initialize mixture coefficients(weights).
	{
		double sum = 0.0;
		size_t c;
		for (size_t k = 0; k < K_; ++k)
		{
			for (c = 0; c < C_; ++c)
			{
				alphas_(k, c) = (double)std::rand() / RAND_MAX;
				sum += alphas_(k, c);
			}
			for (c = 0; c < C_; ++c)
				alphas_(k, c) /= sum;
		}
	}

	// initialize the parameters of observation density.
	const std::size_t numLowerBound = lowerBoundsOfObservationDensity.size();
	const std::size_t numUpperBound = upperBoundsOfObservationDensity.size();

	const std::size_t numParameters = K_ * C_ * P_ + K_ * C_;  // the total number of parameters of observation density.

	assert(numLowerBound == numUpperBound);
	assert(1 == numLowerBound || numParameters == numLowerBound);

	if (1 == numLowerBound)
	{
		const double lb = lowerBoundsOfObservationDensity[0], ub = upperBoundsOfObservationDensity[0];
		size_t k, c, p;
		for (k = 0; k < K_; ++k)
			for (c = 0; c < C_; ++c)
			{
				for (p = 0; p < P_; ++p)
					coeffs_[k][c](p) = ((double)std::rand() / RAND_MAX) * (ub - lb) + lb;
				// TODO [check] >> all variances have to be positive.
				sigmas_[k][c] = ((double)std::rand() / RAND_MAX) * (ub - lb) + lb;
			}
 	}
	else if (numParameters == numLowerBound)
	{
		size_t k, c, p, idx = 0;
		for (k = 0; k < K_; ++k)
			for (c = 0; c < C_; ++c)
				for (p = 0; p < P_; ++p, ++idx)
					coeffs_[k][c](p) = ((double)std::rand() / RAND_MAX) * (upperBoundsOfObservationDensity[idx] - lowerBoundsOfObservationDensity[idx]) + lowerBoundsOfObservationDensity[idx];
		for (k = 0; k < K_; ++k)
			for (c = 0; c < C_; ++c, ++idx)
				// TODO [check] >> all variances have to be positive.
				sigmas_[k][c] = ((double)std::rand() / RAND_MAX) * (upperBoundsOfObservationDensity[idx] - lowerBoundsOfObservationDensity[idx]) + lowerBoundsOfObservationDensity[idx];
	}

	for (size_t k = 0; k < K_; ++k)
		for (size_t c = 0; c < C_; ++c)
		{
			// all variances have to be positive.
			if (sigmas_[k][c] < 0.0)
				sigmas_[k][c] = -sigmas_[k][c];
		}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be positive.
}

}  // namespace swl
