#include "swl/Config.h"
#include "swl/rnd_util/ArHmmWithMultivariateNormalMixtureObservations.h"
#include "RndUtilLocalApi.h"
#include "swl/math/MathConstant.h"
#include <gsl/gsl_rng.h>
#include <gsl/gsl_randist.h>
#include <boost/numeric/ublas/matrix_proxy.hpp>
#include <boost/numeric/ublas/blas.hpp>
#include <boost/numeric/ublas/lu.hpp>
#include <boost/math/constants/constants.hpp>
#include <numeric>
#include <ctime>
#include <stdexcept>
#include <cassert>


#if defined(_DEBUG) && defined(__SWL_CONFIG__USE_DEBUG_NEW)
#include "swl/ResourceLeakageCheck.h"
#define new DEBUG_NEW
#endif


namespace swl {

// [ref] swl/src/rnd_util/RndUtilLocalApi.cpp.
double det_and_inv_by_lu(const boost::numeric::ublas::matrix<double> &m, boost::numeric::ublas::matrix<double> &inv);
bool solve_linear_equations_by_lu(const boost::numeric::ublas::matrix<double> &m, boost::numeric::ublas::vector<double> &x);

ArHmmWithMultivariateNormalMixtureObservations::ArHmmWithMultivariateNormalMixtureObservations(const size_t K, const size_t D, const size_t C, const size_t P)
: base_type(K, D, C), P_(P), coeffs_(boost::extents[K][C]), sigmas_(boost::extents[K][C]),  // 0-based index.
  coeffs_conj_(),
  r_(NULL)
{
	assert(P_ > 0);

	for (size_t k = 0; k < K; ++k)
		for (size_t c = 0; c < C; ++c)
		{
			coeffs_[k][c] = dmatrix_type(D, P, 0.0);
			sigmas_[k][c] = dvector_type(D, 0.0);
		}
}

ArHmmWithMultivariateNormalMixtureObservations::ArHmmWithMultivariateNormalMixtureObservations(const size_t K, const size_t D, const size_t C, const size_t P, const dvector_type &pi, const dmatrix_type &A, const dmatrix_type &alphas, const boost::multi_array<dmatrix_type, 2> &coeffs, const boost::multi_array<dvector_type, 2> &sigmas)
: base_type(K, D, C, pi, A, alphas), P_(P), coeffs_(coeffs), sigmas_(sigmas),
  coeffs_conj_(),
  r_(NULL)
{
	assert(P_ > 0);
}

ArHmmWithMultivariateNormalMixtureObservations::ArHmmWithMultivariateNormalMixtureObservations(const size_t K, const size_t D, const size_t C, const size_t P, const dvector_type *pi_conj, const dmatrix_type *A_conj, const dmatrix_type *alphas_conj, const boost::multi_array<dmatrix_type, 2> *coeffs_conj)
: base_type(K, D, C, pi_conj, A_conj, alphas_conj), P_(P), coeffs_(boost::extents[K][C]), sigmas_(boost::extents[K][C]),
  coeffs_conj_(coeffs_conj),
  r_(NULL)
{
	// FIXME [modify] >>
	throw std::runtime_error("not yet implemented");

	assert(P_ > 0);

	for (size_t k = 0; k < K; ++k)
		for (size_t c = 0; c < C; ++c)
		{
			coeffs_[k][c] = dmatrix_type(D, P, 0.0);
			sigmas_[k][c] = dvector_type(D, 0.0);
		}
}

ArHmmWithMultivariateNormalMixtureObservations::~ArHmmWithMultivariateNormalMixtureObservations()
{
}

void ArHmmWithMultivariateNormalMixtureObservations::doEstimateObservationDensityParametersByML(const size_t N, const unsigned int state, const dmatrix_type &observations, const dmatrix_type &gamma, const double denominatorA)
{
	const double eps = 1e-50;
	size_t c, n;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	dmatrix_type zeta(N, C_, 0.0);
	{
		double denominator;
		double val;
		dmatrix_type inv(D_, D_);
		for (n = 0; n < N; ++n)
		{
			//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);

			denominator = 0.0;
			for (c = 0; c < C_; ++c)
			{
				//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
				val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observations));

				zeta(n, c) = val;
				denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
			}

			if (denominator < eps)
			{
				// FIXME [check] >>
				//	because responsibilities, gamma(y_nc) means membership, the values may become zero if the corresponding mixture model doesn't generate a sample.
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.0;
			}
			else
			{
#if 0
				val = 0.999 * gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.001 + val * zeta(n, c);
#else
				val = gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) *= val;
#endif
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	const double sumGamma = denominatorA + gamma(N-1, state);
	assert(std::fabs(sumGamma) >= eps);
	const double factorAlpha = 0.999 / sumGamma;

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	dmatrix_type autocovariance(N, P_ + 1, 0.0);  // autocovariance function.
	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double meanWinObs;
	double sumZeta;
	for (size_t d = 0; d < D_; ++d)
	{
		const dvector_type &observation_d = boost::numeric::ublas::matrix_column<const dmatrix_type>(observations, d);

		// calculate autocovariance functions.
		autocovariance.clear();
		for (n = 0; n < N; ++n)
		{
			for (w = -W; w <= W; ++w)
			{
				// TODO [check] >> which one is better?
				//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(N)) ? 0.0 : observation_d(n + w);
				winObs(w + W) = int(n) + w < 0 ? observation_d(0) : (int(n) + w >= int(N) ? observation_d(N - 1) : observation_d(n + w));
			}

			meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
			// zero mean observations.
			for (j = 0; j < L; ++j)
				winObs(j) -= meanWinObs;

			for (p = 0; p <= P_; ++p)
				for (j = 0; j < L - p; ++j)
					autocovariance(n, p) += winObs(j) * winObs(j + p);
		}

		autocovariance_bar.clear();
		for (c = 0; c < C_; ++c)
		{
			sumZeta = 0.0;
			for (n = 0; n < N; ++n)
				sumZeta += zeta(n, c);
			assert(std::fabs(sumZeta) >= eps);

			// reestimate mixture coefficients(weights).
			alphas_(state, c) = 0.001 + factorAlpha * sumZeta;

			autocovariance_bar.clear();
			for (p = 0; p <= P_; ++p)
			{
				for (n = 0; n < N; ++n)
					autocovariance_bar(p) += zeta(n, c) * autocovariance(n, p);
				autocovariance_bar(p) /= sumZeta;
			}

			// reestimate the autoregression coefficients.
			for (p = 0; p < P_; ++p)
			{
				x(p) = autocovariance_bar(p + 1);
				for (j = 0; j < p; ++j)
					A(p, j) = autocovariance_bar(p - j);
				for (j = p; j < P_; ++j)
					A(p, j) = autocovariance_bar(j - p);
			}

			if (solve_linear_equations_by_lu(A, x))
			{
#if defined(__GNUC__)
                // FIXME [check] >> is it correct?
				boost::numeric::ublas::matrix_row<dmatrix_type> coeff(boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d));
#else
				boost::numeric::ublas::matrix_row<dmatrix_type> &coeff = boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d);
#endif
				coeff = x;

				// reestimate the variances of the input noise process.
				double &sigma2 = sigmas_[state][c](d);
				sigma2 = autocovariance_bar(0);
				for (p = 0; p < P_; ++p)
					sigma2 -= coeff(p) * autocovariance_bar(p + 1);
				assert(sigma2 > 0.0);
			}
			else
			{
				assert(false);
			}
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be symmetric positive definite.
}

void ArHmmWithMultivariateNormalMixtureObservations::doEstimateObservationDensityParametersByML(const std::vector<size_t> &Ns, const unsigned int state, const std::vector<dmatrix_type> &observationSequences, const std::vector<dmatrix_type> &gammas, const size_t R, const double denominatorA)
{
	const double eps = 1e-50;
	size_t c, n, r;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	std::vector<dmatrix_type> zetas;
	zetas.reserve(R);
	for (r = 0; r < R; ++r)
		zetas.push_back(dmatrix_type(Ns[r], C_, 0.0));

	{
		double denominator;
		double val;
		dmatrix_type inv(D_, D_);
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &gammar = gammas[r];
			dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);

				denominator = 0.0;
				for (c = 0; c < C_; ++c)
				{
					//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
					val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr));

					zetar(n, c) = val;
					denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
				}

				if (denominator < eps)
				{
					// FIXME [check] >>
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.0;
				}
				else
				{
#if 0
					val = 0.999 * gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.001 + val * zetar(n, c);
#else
					val = gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) *= val;
#endif
				}
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	double sumGamma = denominatorA;
	for (r = 0; r < R; ++r)
		sumGamma += gammas[r](Ns[r]-1, state);
	assert(std::fabs(sumGamma) >= eps);
	const double factorAlpha = 0.999 / sumGamma;

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	std::vector<dmatrix_type> autocovariances(R);  // autocovariance functions.
	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double meanWinObs;
	double sumZeta;
	for (size_t d = 0; d < D_; ++d)
	{
		for (r = 0; r < R; ++r)
		{
			assert(Ns[r] > P_);

			const dmatrix_type &observationr = observationSequences[r];
			//const dmatrix_type &zetar = zetas[r];
			dmatrix_type &autocovariancer = autocovariances[r];

			const dvector_type &observationr_d = boost::numeric::ublas::matrix_column<const dmatrix_type>(observationr, d);

			// calculate autocovariance functions.
			autocovariancer.resize(Ns[r], P_ + 1, false);
			autocovariancer.clear();
			for (n = 0; n < Ns[r]; ++n)
			{
				for (w = -W; w <= W; ++w)
				{
					// TODO [check] >> which one is better?
					//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(Ns[r])) ? 0.0 : observationr_d(n + w);
					winObs(w + W) = int(n) + w < 0 ? observationr_d(0) : (int(n) + w >= int(Ns[r]) ? observationr_d(Ns[r] - 1) : observationr_d(n + w));
				}

				meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
				// zero mean observations.
				for (j = 0; j < L; ++j)
					winObs(j) -= meanWinObs;

				for (p = 0; p <= P_; ++p)
					for (j = 0; j < L - p; ++j)
						autocovariancer(n, p) += winObs(j) * winObs(j + p);
			}
		}

		for (c = 0; c < C_; ++c)
		{
			sumZeta = 0.0;
			for (r = 0; r < R; ++r)
			{
				const dmatrix_type &zetar = zetas[r];

				for (n = 0; n < Ns[r]; ++n)
					sumZeta += zetar(n, c);
			}
			assert(std::fabs(sumZeta) >= eps);

			// reestimate mixture coefficients(weights).
			alphas_(state, c) = 0.001 + factorAlpha * sumZeta;

			autocovariance_bar.clear();
			for (p = 0; p <= P_; ++p)
			{
				for (r = 0; r < R; ++r)
				{
					const dmatrix_type &zetar = zetas[r];
					const dmatrix_type &autocovariancer = autocovariances[r];

					for (n = 0; n < Ns[r]; ++n)
						autocovariance_bar(p) += zetar(n, c) * autocovariancer(n, p);
				}

				autocovariance_bar(p) /= sumZeta;
			}

			// reestimate the autoregression coefficients.
			for (p = 0; p < P_; ++p)
			{
				x(p) = autocovariance_bar(p + 1);
				for (j = 0; j < p; ++j)
					A(p, j) = autocovariance_bar(p - j);
				for (j = p; j < P_; ++j)
					A(p, j) = autocovariance_bar(j - p);
			}

			if (solve_linear_equations_by_lu(A, x))
			{
#if defined(__GNUC__)
                // FIXME [check] >> is it correct?
				boost::numeric::ublas::matrix_row<dmatrix_type> coeff(boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d));
#else
				boost::numeric::ublas::matrix_row<dmatrix_type> &coeff = boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d);
#endif
				coeff = x;

				// reestimate the variances of the input noise process.
				double &sigma2 = sigmas_[state][c](d);
				sigma2 = autocovariance_bar(0);
				for (p = 0; p < P_; ++p)
					sigma2 -= coeff(p) * autocovariance_bar(p + 1);
				assert(sigma2 > 0.0);
			}
			else
			{
				assert(false);
			}
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be symmetric positive definite.
}

void ArHmmWithMultivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingConjugatePrior(const size_t N, const unsigned int state, const dmatrix_type &observations, const dmatrix_type &gamma, const double denominatorA)
{
	// FIXME [modify] >>
	throw std::runtime_error("not yet implemented");

#if 0
	const double eps = 1e-50;
	size_t c, n;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	dmatrix_type zeta(N, C_, 0.0);
	{
		double denominator;
		double val;
		dmatrix_type inv(D_, D_);
		for (n = 0; n < N; ++n)
		{
			//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);

			denominator = 0.0;
			for (c = 0; c < C_; ++c)
			{
				//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
				val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observations));

				zeta(n, c) = val;
				denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
			}

			if (denominator < eps)
			{
				// FIXME [check] >>
				//	because responsibilities, gamma(y_nc) means membership, the values may become zero if the corresponding mixture model doesn't generate a sample.
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.0;
			}
			else
			{
#if 0
				val = 0.999 * gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.001 + val * zeta(n, c);
#else
				val = gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) *= val;
#endif
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	const double sumGamma = denominatorA + gamma(N-1, state);
	//assert(std::fabs(sumGamma) >= eps);
	double denominatorAlpha0 = -double(C_);
	for (c = 0; c < C_; ++c)
		denominatorAlpha0 += (*alphas_conj_)(state, c);
	const double factorAlpha = 0.999 / (sumGamma + denominatorAlpha0);

	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		sumZeta = 0.0;
		for (n = 0; n < N; ++n)
			sumZeta += zeta(n, c);
		//assert(std::fabs(sumZeta) >= eps);

		// reestimate mixture coefficients(weights).
		alphas_(state, c) = 0.001 + factorAlpha * (sumZeta + (*alphas_conj_)(state, c) - 1.0);

		// reestimate symbol prob in each state.
		dvector_type &mu = mus_[state][c];
		mu = (*betas_conj_)(state, c) * (*mus_conj_)[state][c];
		for (n = 0; n < N; ++n)
			mu += zeta(n, c) * boost::numeric::ublas::matrix_row<const dmatrix_type>(observations, n);
		//mu = mu * (0.999 / (sumZeta + (*betas_conj_)(state, c))) + boost::numeric::ublas::scalar_vector<double>(mu.size(), 0.001);
		mu = mu * (0.999 / (sumZeta + (*betas_conj_)(state, c))) + boost::numeric::ublas::scalar_vector<double>(D_, 0.001);

		//
		dmatrix_type &sigma = sigmas_[state][c];
		sigma = (*sigmas_conj_)[state][c];
		boost::numeric::ublas::blas_2::sr(sigma, (*betas_conj_)(state, c), mu - (*mus_conj_)[state][c]);
		for (n = 0; n < N; ++n)
			boost::numeric::ublas::blas_2::sr(sigma, gamma(n, state), boost::numeric::ublas::matrix_row<const dmatrix_type>(observations, n) - mu);
		sigma = 0.5 * (sigma + boost::numeric::ublas::trans(sigma));

		//sigma = sigma * (0.999 / (sumZeta + (*nus_conj_)(state, c) - D_)) + boost::numeric::ublas::scalar_matrix<double>(sigma.size1(), sigma.size2(), 0.001);
		sigma = sigma * (0.999 / (sumZeta + (*nus_conj_)(state, c) - D_)) + boost::numeric::ublas::scalar_matrix<double>(D_, D_, 0.001);
	}

	// POSTCONDITIONS [] >>
	//	-. all covariance matrices have to be symmetric positive definite.
#endif
}

void ArHmmWithMultivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingConjugatePrior(const std::vector<size_t> &Ns, const unsigned int state, const std::vector<dmatrix_type> &observationSequences, const std::vector<dmatrix_type> &gammas, const size_t R, const double denominatorA)
{
	// FIXME [modify] >>
	throw std::runtime_error("not yet implemented");

#if 0
	const double eps = 1e-50;
	size_t c, n, r;

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	std::vector<dmatrix_type> zetas;
	zetas.reserve(R);
	for (r = 0; r < R; ++r)
		zetas.push_back(dmatrix_type(Ns[r], C_, 0.0));

	{
		double denominator;
		double val;
		dmatrix_type inv(D_, D_);
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &gammar = gammas[r];
			dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);

				denominator = 0.0;
				for (c = 0; c < C_; ++c)
				{
					//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
					val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr));

					zetar(n, c) = val;
					denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
				}

				if (denominator < eps)
				{
					// FIXME [check] >>
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.0;
				}
				else
				{
#if 0
					val = 0.999 * gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.001 + val * zetar(n, c);
#else
					val = gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) *= val;
#endif
				}
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	double sumGamma = denominatorA;
	for (r = 0; r < R; ++r)
		sumGamma += gammas[r](Ns[r]-1, state);
	//assert(std::fabs(sumGamma) >= eps);
	double denominatorAlpha0 = -double(C_);
	for (c = 0; c < C_; ++c)
		denominatorAlpha0 += (*alphas_conj_)(state, c);
	const double factorAlpha = 0.999 / (sumGamma + denominatorAlpha0);

	double sumZeta;
	for (c = 0; c < C_; ++c)
	{
		sumZeta = 0.0;
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
				sumZeta += zetar(n, c);
		}
		//assert(std::fabs(sumZeta) >= eps);

		// reestimate mixture coefficients(weights).
		alphas_(state, c) = 0.001 + factorAlpha * (sumZeta + (*alphas_conj_)(state, c) - 1.0);

		// reestimate observation(emission) distribution in each state.
		dvector_type &mu = mus_[state][c];
		mu = (*betas_conj_)(state, c) * (*mus_conj_)[state][c];
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
				mu += zetar(n, c) * boost::numeric::ublas::matrix_row<const dmatrix_type>(observationr, n);
		}
		//mu = mu * (0.999 / (sumZeta + (*betas_conj_)(state, c))) + boost::numeric::ublas::scalar_vector<double>(mu.size(), 0.001);
		mu = mu * (0.999 / (sumZeta + (*betas_conj_)(state, c))) + boost::numeric::ublas::scalar_vector<double>(D_, 0.001);

		//
		dmatrix_type &sigma = sigmas_[state][c];
		sigma = (*sigmas_conj_)[state][c];
		boost::numeric::ublas::blas_2::sr(sigma, (*betas_conj_)(state, c), mu - (*mus_conj_)[state][c]);
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
				boost::numeric::ublas::blas_2::sr(sigma, zetar(n, c), boost::numeric::ublas::matrix_row<const dmatrix_type>(observationr, n) - mu);
		}
		sigma = 0.5 * (sigma + boost::numeric::ublas::trans(sigma));

		//sigma = sigma * (0.999 / (sumZeta + (*nus_conj_)(state, c) - D_)) + boost::numeric::ublas::scalar_matrix<double>(sigma.size1(), sigma.size2(), 0.001);
		sigma = sigma * (0.999 / (sumZeta + (*nus_conj_)(state, c) - D_)) + boost::numeric::ublas::scalar_matrix<double>(D_, D_, 0.001);
	}

	// POSTCONDITIONS [] >>
	//	-. all covariance matrices have to be symmetric positive definite.
#endif
}

void ArHmmWithMultivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingEntropicPrior(const size_t N, const unsigned int state, const dmatrix_type &observations, const dmatrix_type &gamma, const double z, const bool doesTrimParameter, const double terminationTolerance, const size_t maxIteration, const double /*denominatorA*/)
{
	const double eps = 1e-50;
	size_t c, n;

	dmatrix_type inv(D_, D_);

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	dmatrix_type zeta(N, C_, 0.0);
	{
		double denominator;
		double val;
		for (n = 0; n < N; ++n)
		{
			//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);

			denominator = 0.0;
			for (c = 0; c < C_; ++c)
			{
				// TODO [check] >> we need to check if a component is trimmed or not.
				//	Here, we use the value of alpha in order to check if a component is trimmed or not.
				//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
				val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observations));

				zeta(n, c) = val;
				denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
			}

			if (denominator < eps)
			{
				// FIXME [check] >>
				//	because responsibilities, gamma(y_nc) means membership, the values may become zero if the corresponding mixture model doesn't generate a sample.
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.0;
			}
			else
			{
#if 0
				val = 0.999 * gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) = 0.001 + val * zeta(n, c);
#else
				val = gamma(n, state) / denominator;
				for (c = 0; c < C_; ++c)
					zeta(n, c) *= val;
#endif
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	{
		// compute expected sufficient statistics (ESS).
		std::vector<double> omega(C_, 0.0), theta(C_, 0.0);
		for (c = 0; c < C_; ++c)
		{
			omega[c] = 0.0;
			for (n = 0; n < N; ++n)
				omega[c] += zeta(n, c);
		}

		// reestimate mixture coefficients(weights).
		double entropicMAPLogLikelihood = 0.0;
		const bool retval = computeMAPEstimateOfMultinomialUsingEntropicPrior(omega, z, theta, entropicMAPLogLikelihood, terminationTolerance, maxIteration, false);
		assert(retval);

		// trim mixture coefficients(weights).
		if (doesTrimParameter && std::fabs(z - 1.0) <= eps)
		{
			dmatrix_type prob(N, C_, 0.0);
			for (n = 0; n < N; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observations, n);
				for (c = 0; c < C_; ++c)
				{
					// TODO [check] >> we need to check if a component is trimmed or not.
					//	Here, we use the value of alpha in order to check if a component is trimmed or not.
					//prob(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, obs);
					prob(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, n, observations);
				}
			}

			size_t i;
			double grad;
			bool isNormalized = false;
			double numerator, denominator;
			for (c = 0; c < C_; ++c)
			{
				if (alphas_(state, c) >= eps)  // not yet trimmed.
				{
					grad = 0.0;
					for (n = 0; n < N; ++n)
					{
						numerator = prob(n, c);
						if (std::fabs(numerator) >= eps)
						{
							denominator = 0.0;
							for (i = 0; i < C_; ++i)
								denominator += prob(n, i) * theta[i];

							assert(std::fabs(denominator) >= eps);
							grad += numerator / denominator;
						}
						//else grad += 0.0;
					}

					if (theta[c] <= std::exp(-grad / z))
					{
						theta[c] = 0.0;
						isNormalized = true;
					}
				}
			}

			if (isNormalized)
			{
				double sumTheta = std::accumulate(theta.begin(), theta.end(), 0.0);
				assert(std::fabs(sumTheta) >= eps);
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c] / sumTheta;
			}
			else
			{
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c];
			}
		}
		else
		{
			for (c = 0; c < C_; ++c)
				alphas_(state, c) = theta[c];
		}
	}

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	dmatrix_type autocovariance(N, P_ + 1, 0.0);  // autocovariance function.
	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double meanWinObs;
	double sumZeta;
	for (size_t d = 0; d < D_; ++d)
	{
		const dvector_type &observation_d = boost::numeric::ublas::matrix_column<const dmatrix_type>(observations, d);

		// calculate autocovariance functions.
		autocovariance.clear();
		for (n = 0; n < N; ++n)
		{
			for (w = -W; w <= W; ++w)
			{
				// TODO [check] >> which one is better?
				//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(N)) ? 0.0 : observation_d(n + w);
				winObs(w + W) = int(n) + w < 0 ? observation_d(0) : (int(n) + w >= int(N) ? observation_d(N - 1) : observation_d(n + w));
			}

			meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
			// zero mean observations.
			for (j = 0; j < L; ++j)
				winObs(j) -= meanWinObs;

			for (p = 0; p <= P_; ++p)
				for (j = 0; j < L - p; ++j)
					autocovariance(n, p) += winObs(j) * winObs(j + p);
		}

		autocovariance_bar.clear();
		for (c = 0; c < C_; ++c)
		{
			if (alphas_(state, c) < eps)  // already trimmed.
			{
#if defined(__GNUC__)
                // FIXME [check] >> is it correct?
				boost::numeric::ublas::matrix_row<dmatrix_type> coeff(boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d));
#else
				boost::numeric::ublas::matrix_row<dmatrix_type> &coeff = boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d);
#endif
				coeff = dvector_type(coeff.size(), 0.0);
				sigmas_[state][c](d) = 0.0;
			}
			else
			{
				sumZeta = 0.0;
				for (n = 0; n < N; ++n)
					sumZeta += zeta(n, c);
				assert(std::fabs(sumZeta) >= eps);

				autocovariance_bar.clear();
				for (p = 0; p <= P_; ++p)
				{
					for (n = 0; n < N; ++n)
						autocovariance_bar(p) += zeta(n, c) * autocovariance(n, p);
					autocovariance_bar(p) /= sumZeta;
				}

				// reestimate the autoregression coefficients.
				for (p = 0; p < P_; ++p)
				{
					x(p) = autocovariance_bar(p + 1);
					for (j = 0; j < p; ++j)
						A(p, j) = autocovariance_bar(p - j);
					for (j = p; j < P_; ++j)
						A(p, j) = autocovariance_bar(j - p);
				}

				if (solve_linear_equations_by_lu(A, x))
				{
#if defined(__GNUC__)
                    // FIXME [check] >> is it correct?
					boost::numeric::ublas::matrix_row<dmatrix_type> coeff(boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d));
#else
					boost::numeric::ublas::matrix_row<dmatrix_type> &coeff = boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d);
#endif
					coeff = x;

					// reestimate the variances of the input noise process.
					double &sigma2 = sigmas_[state][c](d);
					sigma2 = autocovariance_bar(0);
					for (p = 0; p < P_; ++p)
						sigma2 -= coeff(p) * autocovariance_bar(p + 1);
					assert(sigma2 > 0.0);
				}
				else
				{
					assert(false);
				}
			}
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all variancews have to be symmetric positive definite.
}

void ArHmmWithMultivariateNormalMixtureObservations::doEstimateObservationDensityParametersByMAPUsingEntropicPrior(const std::vector<size_t> &Ns, const unsigned int state, const std::vector<dmatrix_type> &observationSequences, const std::vector<dmatrix_type> &gammas, const double z, const bool doesTrimParameter, const double terminationTolerance, const size_t maxIteration, const size_t R, const double /*denominatorA*/)
{
	const double eps = 1e-50;
	size_t c, n, r;

	dmatrix_type inv(D_, D_);

	// E-step: evaluate zeta.
	// TODO [check] >> frequent memory reallocation may make trouble.
	std::vector<dmatrix_type> zetas;
	zetas.reserve(R);
	for (r = 0; r < R; ++r)
		zetas.push_back(dmatrix_type(Ns[r], C_, 0.0));

	{
		double denominator;
		double val;
		for (r = 0; r < R; ++r)
		{
			const dmatrix_type &observationr = observationSequences[r];
			const dmatrix_type &gammar = gammas[r];
			dmatrix_type &zetar = zetas[r];

			for (n = 0; n < Ns[r]; ++n)
			{
				//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);

				denominator = 0.0;
				for (c = 0; c < C_; ++c)
				{
					// TODO [check] >> we need to check if a component is trimmed or not.
					//	Here, we use the value of alpha in order to check if a component is trimmed or not.
					//val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, obs));
					val = std::fabs(alphas_(state, c)) < eps ? 0.0 : (alphas_(state, c) * doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr));

					zetar(n, c) = val;
					denominator += val;  // this value can be nearly zero if the observation is not generated by the corresponding mixture model.
				}

				if (denominator < eps)
				{
					// FIXME [check] >>
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.0;
				}
				else
				{
#if 0
					val = 0.999 * gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) = 0.001 + val * zetar(n, c);
#else
					val = gammar(n, state) / denominator;
					for (c = 0; c < C_; ++c)
						zetar(n, c) *= val;
#endif
				}
			}
		}
	}

	// M-step.
	// reestimate observation(emission) distribution in each state.

	{
		// compute expected sufficient statistics (ESS).
		std::vector<double> omega(C_, 0.0), theta(C_, 0.0);
		for (c = 0; c < C_; ++c)
		{
			omega[c] = 0.0;
			for (r = 0; r < R; ++r)
			{
				const dmatrix_type &zetar = zetas[r];
				for (n = 0; n < Ns[r]; ++n)
					omega[c] += zetar(n, c);
			}
		}

		// reestimate mixture coefficients(weights).
		double entropicMAPLogLikelihood = 0.0;
		const bool retval = computeMAPEstimateOfMultinomialUsingEntropicPrior(omega, z, theta, entropicMAPLogLikelihood, terminationTolerance, maxIteration, false);
		assert(retval);

		// trim mixture coefficients(weights).
		if (doesTrimParameter && std::fabs(z - 1.0) <= eps)
		{
			std::vector<dmatrix_type> probs(R);
			for (r = 0; r < R; ++r)
			{
				const size_t &Nr = Ns[r];
				const dmatrix_type &observationr = observationSequences[r];

				dmatrix_type &probr = probs[r];
				probr.resize(Nr, C_);
				for (n = 0; n < Nr; ++n)
				{
					//const boost::numeric::ublas::matrix_row<const dmatrix_type> obs(observationr, n);
					for (c = 0; c < C_; ++c)
					{
						// TODO [check] >> we need to check if a component is trimmed or not.
						//	Here, we use the value of alpha in order to check if a component is trimmed or not.
						//probr(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, obs);
						probr(n, c) = std::fabs(alphas_(state, c)) < eps ? 0.0 : doEvaluateEmissionMixtureComponentProbability(state, c, n, observationr);
					}
				}
			}

			size_t i;
			double grad;
			bool isNormalized = false;
			double numerator, denominator;
			for (c = 0; c < C_; ++c)
			{
				if (alphas_(state, c) >= eps)  // not yet trimmed.
				{
					grad = 0.0;
					for (r = 0; r < R; ++r)
					{
						const size_t &Nr = Ns[r];
						const dmatrix_type &probr = probs[r];
						for (n = 0; n < Nr; ++n)
						{
							numerator = probr(n, c);
							if (std::fabs(numerator) >= eps)
							{
								denominator = 0.0;
								for (i = 0; i < C_; ++i)
									denominator += probr(n, i) * theta[i];

								assert(std::fabs(denominator) >= eps);
								grad += numerator / denominator;
							}
							//else grad += 0.0;
						}
					}

					if (theta[c] <= std::exp(-grad / z))
					{
						theta[c] = 0.0;
						isNormalized = true;
					}
				}
			}

			if (isNormalized)
			{
				double sumTheta = std::accumulate(theta.begin(), theta.end(), 0.0);
				assert(std::fabs(sumTheta) >= eps);
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c] / sumTheta;
			}
			else
			{
				for (c = 0; c < C_; ++c)
					alphas_(state, c) = theta[c];
			}
		}
		else
		{
			for (c = 0; c < C_; ++c)
				alphas_(state, c) = theta[c];
		}
	}

	//
	// TODO [check] >> is it good?
	const int W = int(P_);  // window size.
	const size_t L = 2 * W + 1;  // L consecutive samples of the observation signal.
	dvector_type winObs(L, 0.0);
	assert(L > P_);

	size_t p, j;
	int w;

	std::vector<dmatrix_type> autocovariances(R);  // autocovariance functions.
	dvector_type autocovariance_bar(P_ + 1, 0.0);
	boost::numeric::ublas::matrix<double> A(P_, P_, 0.0);
	boost::numeric::ublas::vector<double> x(P_, 0.0);
	double meanWinObs;
	double sumZeta;
	for (size_t d = 0; d < D_; ++d)
	{
		for (r = 0; r < R; ++r)
		{
			assert(Ns[r] > P_);

			const dmatrix_type &observationr = observationSequences[r];
			//const dmatrix_type &zetar = zetas[r];
			dmatrix_type &autocovariancer = autocovariances[r];

			const dvector_type &observationr_d = boost::numeric::ublas::matrix_column<const dmatrix_type>(observationr, d);

			// calculate autocovariance functions.
			autocovariancer.resize(Ns[r], P_ + 1, false);
			autocovariancer.clear();
			for (n = 0; n < Ns[r]; ++n)
			{
				for (w = -W; w <= W; ++w)
				{
					// TODO [check] >> which one is better?
					//winObs(w + W) = (int(n) + w < 0 || int(n) + w >= int(Ns[r])) ? 0.0 : observationr_d(n + w);
					winObs(w + W) = int(n) + w < 0 ? observationr_d(0) : (int(n) + w >= int(Ns[r]) ? observationr_d(Ns[r] - 1) : observationr_d(n + w));
				}

				meanWinObs = std::accumulate(winObs.begin(), winObs.end(), 0.0) / double(L);
				// zero mean observations.
				for (j = 0; j < L; ++j)
					winObs(j) -= meanWinObs;

				for (p = 0; p <= P_; ++p)
					for (j = 0; j < L - p; ++j)
						autocovariancer(n, p) += winObs(j) * winObs(j + p);
			}
		}

		for (c = 0; c < C_; ++c)
		{
			if (alphas_(state, c) < eps)  // already trimmed.
			{
#if defined(__GNUC__)
                // FIXME [check] >> is it correct?
				boost::numeric::ublas::matrix_row<dmatrix_type> coeff(boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d));
#else
				boost::numeric::ublas::matrix_row<dmatrix_type> &coeff = boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d);
#endif
				coeff = dvector_type(coeff.size(), 0.0);
				sigmas_[state][c](d) = 0.0;
			}
			else
			{
				sumZeta = 0.0;
				for (r = 0; r < R; ++r)
				{
					const dmatrix_type &zetar = zetas[r];

					for (n = 0; n < Ns[r]; ++n)
						sumZeta += zetar(n, c);
				}
				assert(std::fabs(sumZeta) >= eps);

				//
				autocovariance_bar.clear();
				for (p = 0; p <= P_; ++p)
				{
					for (r = 0; r < R; ++r)
					{
						const dmatrix_type &zetar = zetas[r];
						const dmatrix_type &autocovariancer = autocovariances[r];

						for (n = 0; n < Ns[r]; ++n)
							autocovariance_bar(p) += zetar(n, c) * autocovariancer(n, p);
					}

					autocovariance_bar(p) /= sumZeta;
				}

				// reestimate the autoregression coefficients.
				for (p = 0; p < P_; ++p)
				{
					x(p) = autocovariance_bar(p + 1);
					for (j = 0; j < p; ++j)
						A(p, j) = autocovariance_bar(p - j);
					for (j = p; j < P_; ++j)
						A(p, j) = autocovariance_bar(j - p);
				}

				if (solve_linear_equations_by_lu(A, x))
				{
#if defined(__GNUC__)
                    // FIXME [check] >> is it correct?
					boost::numeric::ublas::matrix_row<dmatrix_type> coeff(boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d));
#else
					boost::numeric::ublas::matrix_row<dmatrix_type> &coeff = boost::numeric::ublas::matrix_row<dmatrix_type>(coeffs_[state][c], d);
#endif
					coeff = x;

					// reestimate the variances of the input noise process.
					double &sigma2 = sigmas_[state][c](d);
					sigma2 = autocovariance_bar(0);
					for (p = 0; p < P_; ++p)
						sigma2 -= coeff(p) * autocovariance_bar(p + 1);
					assert(sigma2 > 0.0);
				}
				else
				{
					assert(false);
				}
			}
		}
	}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be symmetric positive definite.
}

double ArHmmWithMultivariateNormalMixtureObservations::doEvaluateEmissionProbability(const unsigned int state, const dvector_type &observation) const
{
	assert(false);

	return base_type::doEvaluateEmissionProbability(state, observation);
}

double ArHmmWithMultivariateNormalMixtureObservations::doEvaluateEmissionProbability(const unsigned int state, const size_t n, const dmatrix_type &observations) const
{
	return base_type::doEvaluateEmissionProbability(state, n, observations);
}

double ArHmmWithMultivariateNormalMixtureObservations::doEvaluateEmissionMixtureComponentProbability(const unsigned int state, const unsigned int component, const dvector_type &observation) const
{
	assert(false);

	dvector_type M(D_, 0.0);
	dmatrix_type S(D_, D_, 0.0);

	//const dmatrix_type &coeff = coeffs_[state][component];
	const dvector_type &sigma2 = sigmas_[state][component];
	for (size_t d = 0; d < D_; ++d)
	{
		for (size_t p = 0; p < P_; ++p)
		{
			// FIXME [check] > is it correct?
			// TODO [check] >> which one is better?
			M(d) = 0.0;  //coeff(d, p) * 0.0;
			//M(d) = coeff(d, p) * observation(d);
		}
		S(d, d) = sigma2(d);
	}

	dmatrix_type inv(S.size1(), S.size2());
	const double det = det_and_inv_by_lu(S, inv);
	assert(det > 0.0);

	const dvector_type x_mu(observation - M);
	return std::exp(-0.5 * boost::numeric::ublas::inner_prod(x_mu, boost::numeric::ublas::prod(inv, x_mu))) / std::sqrt(std::pow(MathConstant::_2_PI, (double)D_) * det);
}

double ArHmmWithMultivariateNormalMixtureObservations::doEvaluateEmissionMixtureComponentProbability(const unsigned int state, const unsigned int component, const size_t n, const dmatrix_type &observations) const
{
	dvector_type M(D_, 0.0);
	dmatrix_type S(D_, D_, 0.0);

	const dmatrix_type &coeff = coeffs_[state][component];
	const dvector_type &sigma2 = sigmas_[state][component];
	for (size_t d = 0; d < D_; ++d)
	{
		for (size_t p = 0; p < P_; ++p)
		{
			// TODO [check] >> which one is better?
			//M(d) = coeff(d, p) * ((int(n) - int(p) - 1 < 0) ? 0.0 : observations(n-p-1, 0));
			M(d) = coeff(d, p) * ((int(n) - int(p) - 1 < 0) ? observations(0, 0) : observations(n-p-1, 0));
		}
		S(d, d) = sigma2(d);
	}

	dmatrix_type inv(S.size1(), S.size2());
	const double det = det_and_inv_by_lu(S, inv);
	assert(det > 0.0);

	const dvector_type x_mu(boost::numeric::ublas::matrix_row<const dmatrix_type>(observations, n) - M);
	return std::exp(-0.5 * boost::numeric::ublas::inner_prod(x_mu, boost::numeric::ublas::prod(inv, x_mu))) / std::sqrt(std::pow(MathConstant::_2_PI, (double)D_) * det);
}

void ArHmmWithMultivariateNormalMixtureObservations::doGenerateObservationsSymbol(const unsigned int state, const size_t n, dmatrix_type &observations) const
{
	assert(NULL != r_);

	// bivariate normal distribution.
	if (2 == D_)
	{
		const double prob = (double)std::rand() / RAND_MAX;

		double accum = 0.0;
		unsigned int component = (unsigned int)C_;
		for (size_t c = 0; c < C_; ++c)
		{
			accum += alphas_(state, c);
			if (prob < accum)
			{
				component = (unsigned int)c;
				break;
			}
		}

		// TODO [check] >>
		if ((unsigned int)C_ == component)
			component = (unsigned int)(C_ - 1);

		//
		dvector_type M(D_, 0.0);
		dmatrix_type S(D_, D_, 0.0);

		const dmatrix_type &coeff = coeffs_[state][component];
		const dvector_type &sigma2 = sigmas_[state][component];
		for (size_t d = 0; d < D_; ++d)
		{
			for (size_t p = 0; p < P_; ++p)
			{
				// TODO [check] >> which one is better?
				//M(d) = coeff(d, p) * ((int(n) - int(p) - 1 < 0) ? 0.0 : observations(n-p-1, d));
				M(d) = coeff(d, p) * ((int(n) - int(p) - 1 < 0) ? observations(0, d) : observations(n-p-1, d));
			}
			S(d, d) = sigma2(d);
		}

		const double sigma_x = std::sqrt(S(0, 0));  // sigma_x = sqrt(cov_xx).
		const double sigma_y = std::sqrt(S(1, 1));  // sigma_y = sqrt(cov_yy).
		const double rho = S(0, 1) / (sigma_x * sigma_y);  // correlation coefficient: rho = cov_xy / (sigma_x * sigma_y).

		double x = 0.0, y = 0.0;
		gsl_ran_bivariate_gaussian(r_, sigma_x, sigma_y, rho, &x, &y);

		observations(n, 0) = M[0] + x;
		observations(n, 1) = M[1] + y;
	}
	else
	{
		throw std::runtime_error("not yet implemented");
	}
}

void ArHmmWithMultivariateNormalMixtureObservations::doInitializeRandomSampleGeneration(const unsigned int seed /*= (unsigned int)-1*/) const
{
	if ((unsigned int)-1 != seed)
	{
		// random number generator algorithms.
		gsl_rng_default = gsl_rng_mt19937;
		//gsl_rng_default = gsl_rng_taus;
		gsl_rng_default_seed = seed;
	}

	const gsl_rng_type *T = gsl_rng_default;
	r_ = gsl_rng_alloc(T);
}

void ArHmmWithMultivariateNormalMixtureObservations::doFinalizeRandomSampleGeneration() const
{
	gsl_rng_free(r_);
	r_ = NULL;
}

bool ArHmmWithMultivariateNormalMixtureObservations::doReadObservationDensity(std::istream &stream)
{
	std::string dummy;
	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "ar") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "ar") != 0)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "multivariate") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "multivariate") != 0)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "normal") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "normal") != 0)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "mixture:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "mixture:") != 0)
#endif
		return false;

	// TODO [check] >>
	size_t C;
	stream >> dummy >> C;  // the number of mixture components.
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "C=") != 0 || C_ != C)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "C=") != 0 || C_ != C)
#endif
		return false;

	// TODO [check] >>
	size_t P;
	stream >> dummy >> P;  // the order of autoregressive model.
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "P=") != 0 || P_ != P)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "P=") != 0 || P_ != P)
#endif
		return false;

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "alpha:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "alpha:") != 0)
#endif
		return false;

	size_t k, c, d, p;

	// K x C.
	for (k = 0; k < K_; ++k)
		for (c = 0; c < C_; ++c)
			stream >> alphas_(k, c);

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "coeff:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "coeff:") != 0)
#endif
		return false;

	// K x C x D x P.
	for (k = 0; k < K_; ++k)
		for (c = 0; c < C_; ++c)
		{
			dmatrix_type &coeff = coeffs_[k][c];

			for (d = 0; d < D_; ++d)
				for (p = 0; p < P_; ++p)
					stream >> coeff(d, p);
		}

	stream >> dummy;
#if defined(__GNUC__)
	if (strcasecmp(dummy.c_str(), "sigma:") != 0)
#elif defined(_MSC_VER)
	if (_stricmp(dummy.c_str(), "sigma:") != 0)
#endif
		return false;

	// K x C x D.
	for (k = 0; k < K_; ++k)
		for (c = 0; c < C_; ++c)
		{
			dvector_type &sigma2 = sigmas_[k][c];

			for (d = 0; d < D_; ++d)
				stream >> sigma2(d);
		}

	return true;
}

bool ArHmmWithMultivariateNormalMixtureObservations::doWriteObservationDensity(std::ostream &stream) const
{
	stream << "ar multivariate normal mixture:" << std::endl;

	stream << "C= " << C_ << std::endl;  // the number of mixture components.

	stream << "P= " << P_ << std::endl;  // the order of autoregressive model.

	size_t k, c, d, p;

	// K x C.
	stream << "alpha:" << std::endl;
	for (k = 0; k < K_; ++k)
	{
		for (c = 0; c < C_; ++c)
			stream << alphas_(k, c) << ' ';
		stream << std::endl;
	}

	// K x C x D x P.
	stream << "coeff:" << std::endl;
	for (k = 0; k < K_; ++k)
	{
		for (c = 0; c < C_; ++c)
		{
			const dmatrix_type &coeff = coeffs_[k][c];

			for (d = 0; d < D_; ++d)
			{
				for (p = 0; p < D_; ++p)
					stream << coeff(d, p) << ' ';
				stream << "  ";
			}
			stream << std::endl;
		}
		stream << std::endl;
	}

	// K x C x D.
	stream << "sigma:" << std::endl;
	for (k = 0; k < K_; ++k)
	{
		for (c = 0; c < C_; ++c)
		{
			const dvector_type &sigma2 = sigmas_[k][c];

			for (d = 0; d < D_; ++d)
				stream << sigma2(d) << ' ';
			stream << std::endl;
		}
		stream << std::endl;
	}

	return true;
}

void ArHmmWithMultivariateNormalMixtureObservations::doInitializeObservationDensity(const std::vector<double> &lowerBoundsOfObservationDensity, const std::vector<double> &upperBoundsOfObservationDensity)
{
	// PRECONDITIONS [] >>
	//	-. std::srand() has to be called before this function is called.

	// initialize mixture coefficients(weights).
	{
		double sum;
		size_t c;
		for (size_t k = 0; k < K_; ++k)
		{
			sum = 0.0;
			for (c = 0; c < C_; ++c)
			{
				alphas_(k, c) = (double)std::rand() / RAND_MAX;
				sum += alphas_(k, c);
			}
			for (c = 0; c < C_; ++c)
				alphas_(k, c) /= sum;
		}
	}

	// initialize the parameters of observation density.
	const std::size_t numLowerBound = lowerBoundsOfObservationDensity.size();
	const std::size_t numUpperBound = upperBoundsOfObservationDensity.size();

	const std::size_t numParameters = K_ * C_ * D_ * P_ + K_ * C_ * D_;  // the total number of parameters of observation density.

	assert(numLowerBound == numUpperBound);
	assert(1 == numLowerBound || numParameters == numLowerBound);

	if (1 == numLowerBound)
	{
		const double lb = lowerBoundsOfObservationDensity[0], ub = upperBoundsOfObservationDensity[0];
		size_t k, c, d, p;
		for (k = 0; k < K_; ++k)
			for (c = 0; c < C_; ++c)
			{
				dmatrix_type &coeff = coeffs_[k][c];
				dvector_type &sigma2 = sigmas_[k][c];
				for (d = 0; d < D_; ++d)
				{
					for (p = 0; p < P_; ++p)
						coeff(d, p) = ((double)std::rand() / RAND_MAX) * (ub - lb) + lb;
					sigma2(d) = ((double)std::rand() / RAND_MAX) * (ub - lb) + lb;
				}
			}
 	}
	else if (numParameters == numLowerBound)
	{
		size_t k, c, d, p, idx = 0;
		for (k = 0; k < K_; ++k)
			for (c = 0; c < C_; ++c)
			{
				dmatrix_type &coeff = coeffs_[k][c];
				for (d = 0; d < D_; ++d)
					for (p = 0; p < P_; ++p, ++idx)
						coeff(d, p) = ((double)std::rand() / RAND_MAX) * (upperBoundsOfObservationDensity[idx] - lowerBoundsOfObservationDensity[idx]) + lowerBoundsOfObservationDensity[idx];
			}
		for (k = 0; k < K_; ++k)
			for (c = 0; c < C_; ++c)
			{
				dvector_type &sigma2 = sigmas_[k][c];
				for (d = 0; d < D_; ++d, ++idx)
					sigma2(d) = ((double)std::rand() / RAND_MAX) * (upperBoundsOfObservationDensity[idx] - lowerBoundsOfObservationDensity[idx]) + lowerBoundsOfObservationDensity[idx];
			}
	}

	for (size_t k = 0; k < K_; ++k)
		for (size_t c = 0; c < C_; ++c)
		{
			dvector_type &sigma2 = sigmas_[k][c];

			// all variances have to be symmetric positive definite.
			for (size_t d = 0; d < D_; ++d)
				if (sigma2(d) < 0.0)
					sigma2(d) = -sigma2(d);
		}

	// POSTCONDITIONS [] >>
	//	-. all variances have to be symmetric positive definite.
}

}  // namespace swl
